{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Wall Street Journal Logistic News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os import getcwd, path as base_path\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "import re\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "PROJECT_PATH = getcwd()[:getcwd().find(\"notebooks\")][:-1]\n",
    "path.append(f\"{PROJECT_PATH}\\\\src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (get_words_contractions, \n",
    "                   find_urls_in_text, \n",
    "                   get_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"wsj_tweet_scrapping\"\n",
    "PROJECT_PATH = getcwd()[: getcwd().find(PROJECT_NAME) + len(PROJECT_NAME)]\n",
    "DATA_PATH = f\"{PROJECT_PATH}\\\\data\"\n",
    "EXCEL_PATH = f\"{DATA_PATH}\\\\excel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_year = 2011\n",
    "to_year = 2020\n",
    "news_df = pd.read_excel(f\"{EXCEL_PATH}\\\\wsj_logistics_news_{from_year}-{to_year}.xlsx\")\n",
    "news_df.dropna(inplace=True)\n",
    "news_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2473 news articles.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>url</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>Today’s newsletter - Cleaning Vehicle Emissi...</td>\n",
       "      <td>https://on.wsj.com/2EuJDdb</td>\n",
       "      <td>\\n\\tNews and analysis on the world of logisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>FedEx's Christmas in July; Probing Nikola Cl...</td>\n",
       "      <td>https://on.wsj.com/2Fsewzz</td>\n",
       "      <td>\\n\\tNews and analysis on the world of logisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-14</td>\n",
       "      <td>Shipping’s E-Commerce Drive; Short-Selling N...</td>\n",
       "      <td>https://on.wsj.com/3iv2qDW</td>\n",
       "      <td>\\n\\tNews and analysis on the world of logisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>Brexit’s New Alarms; Railroad Offloads Bidde...</td>\n",
       "      <td>https://on.wsj.com/3inRU1m</td>\n",
       "      <td>\\n\\tNews and analysis on the world of logisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-23</td>\n",
       "      <td>Airlines Turn to Freight; Driving Robot Truc...</td>\n",
       "      <td>https://on.wsj.com/32TvO1a</td>\n",
       "      <td>\\n\\tNews and analysis on the world of logisti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              tweet  \\\n",
       "0 2020-09-24    Today’s newsletter - Cleaning Vehicle Emissi...   \n",
       "1 2020-09-16    FedEx's Christmas in July; Probing Nikola Cl...   \n",
       "2 2020-09-14    Shipping’s E-Commerce Drive; Short-Selling N...   \n",
       "3 2020-09-10    Brexit’s New Alarms; Railroad Offloads Bidde...   \n",
       "4 2020-09-23    Airlines Turn to Freight; Driving Robot Truc...   \n",
       "\n",
       "                          url  \\\n",
       "0  https://on.wsj.com/2EuJDdb   \n",
       "1  https://on.wsj.com/2Fsewzz   \n",
       "2  https://on.wsj.com/3iv2qDW   \n",
       "3  https://on.wsj.com/3inRU1m   \n",
       "4  https://on.wsj.com/32TvO1a   \n",
       "\n",
       "                                                news  \n",
       "0   \\n\\tNews and analysis on the world of logisti...  \n",
       "1   \\n\\tNews and analysis on the world of logisti...  \n",
       "2   \\n\\tNews and analysis on the world of logisti...  \n",
       "3   \\n\\tNews and analysis on the world of logisti...  \n",
       "4   \\n\\tNews and analysis on the world of logisti...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Found {len(news_df)} news articles.\")\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"news\"] = news_df[\"news\"].astype(str)\n",
    "if \"clean_news\" in news_df.columns:\n",
    "    news_df.drop(\"clean_news\", axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_sentence_from_identifier(text : str, identifiers : List, split_at : str = \". \"):\n",
    "    \"\"\"Identify sentences in text with identifiers.\"\"\"\n",
    "    sentences = text.split(split_at)\n",
    "    if len(sentences) == 0:\n",
    "        return\n",
    "    \n",
    "    identified_sentences = []\n",
    "    for sentence in sentences:\n",
    "        for identifier in identifiers:\n",
    "            if identifier in sentence and sentence not in identified_sentences:\n",
    "                identified_sentences.append(sentence.strip() + \".\")\n",
    "    return identified_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_news(news, \n",
    "               remove_stopwords : bool = False, \n",
    "               process_contractions : bool = False, \n",
    "               remove_special_chars : bool = False,\n",
    "               remove_all_special_chars : bool = False,\n",
    "               remove_single_alpha_char_word : bool = False):\n",
    "    \n",
    "    \"\"\"Can be used to clean news article.\"\"\"\n",
    "    \n",
    "    # to remove urls within text\n",
    "    urls_to_remove = find_urls_in_text(news)     \n",
    "    for url in urls_to_remove:\n",
    "        news = news.replace(url, \" \")\n",
    "    \n",
    "    # to remove unwanted text\n",
    "    discard_texts = [\"\\n\", \"\\t\", \"\\r\", \"\\xa0\"] \n",
    "    for word in discard_texts: \n",
    "        news = news.replace(word, \" \")\n",
    "        \n",
    "    # truncating articles from start of below identifiers\n",
    "    truncate_from_identifiers = [\"Sign up here.\", \n",
    "                                 \"Write to\"]\n",
    "    for identifier in truncate_from_identifiers:\n",
    "        idx = re.search(identifier, news)\n",
    "        if idx:\n",
    "            idx = idx.span()[0]\n",
    "            news = news[:idx]\n",
    "    \n",
    "    \n",
    "    remove_sentences_with_identifiers = [\"News and analysis on the world\", \n",
    "                                         \"Trouble viewing this email?\",\n",
    "                                         \"Enter News, Quotes\",\n",
    "                                         \"Read more\",\n",
    "                                        \"Get your supply chain ready for\", \n",
    "                                        \"Explore insights on how to master\", \n",
    "                                        \"Follow the WSJ Logistics Report\",\n",
    "                                        \"Sign up: With one click, get this newsletter\"\n",
    "                                        \"delivered to your inbox\",\n",
    "                                        \"Trouble viewing this email? View in web browser\", \n",
    "                                        \"Write to him at paul.page@wsj.com. \",\n",
    "                                         \"is editor of WSJ\", \n",
    "                                         \"Write to him\",\n",
    "                                         \n",
    "                   ]\n",
    "    remove_sentences = identify_sentence_from_identifier(text=news,\n",
    "                                                         identifiers=remove_sentences_with_identifiers)\n",
    "    for sentence in remove_sentences:\n",
    "        news = news.replace(sentence, \" \")\n",
    "            \n",
    "#     news = news.lower()\n",
    "\n",
    "#     if process_contractions:\n",
    "#         cleaned_news = \"\"\n",
    "#         contractions = get_words_contractions()\n",
    "#         for word in news.split():\n",
    "#             if word in contractions:\n",
    "#                 word = contractions[word]\n",
    "#             cleaned_news += \" \" + word\n",
    "#         news = cleaned_news\n",
    "        \n",
    "#     if remove_stopwords:\n",
    "#         cleaned_news = \"\"\n",
    "#         stopwords = get_stopwords(lang=\"english\")\n",
    "#         for word in news.split():\n",
    "#             if word not in stopwords:\n",
    "#                 cleaned_news += \" \" + word\n",
    "#         news = cleaned_news\n",
    "         \n",
    "#     if remove_special_chars:\n",
    "#         if remove_all_special_chars:\n",
    "#             chars_to_keep = \"abcdefghijklmnopqrstuvwxyz1234567890 \"\n",
    "#         else:\n",
    "#             chars_to_keep = \"abcdefghijklmnopqrstuvwxyz1234567890 .,''’\"\n",
    "#         news = \"\".join([char_ if char_ in chars_to_keep else \" \" for char_ in news])\n",
    "    \n",
    "#     if remove_single_alpha_char_word:\n",
    "#         cleaned_news = \"\"\n",
    "#         for word in news.split():\n",
    "#             if word.isdigit() or len(word) > 1:\n",
    "#                 cleaned_news += \" \" + word\n",
    "#         news = cleaned_news\n",
    "\n",
    "    news = \" \".join(news.split())     \n",
    "\n",
    "    return news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some minor specific cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove sentences with word \"PHOTO\" (photographer name, etc)\n",
    "tags_split = {\"PHOTO\" : \"\\n\"} # {tag : split_identifier}\n",
    "clean_texts = []\n",
    "for text in news_df[\"news\"]:\n",
    "    for tag in tags_split.keys():\n",
    "        sentences = identify_sentence_from_identifier(text = text, identifiers=[tag])\n",
    "        remove_sentences_ = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for sub_sentence in sentence.split(tags_split[tag]):\n",
    "                if tag in sub_sentence:\n",
    "                    remove_sentences_.append(sub_sentence)\n",
    "    if len(remove_sentences_) > 0:\n",
    "        for sentence in remove_sentences_:\n",
    "            text = text.replace(sentence, \" \")\n",
    "    clean_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove news channel from news (helps in filtering out news channel name from company names identified in news articles)\n",
    "all_news_channels = []\n",
    "for text in clean_texts:\n",
    "    channels = np.unique(re.findall(r\"\\(.*?\\)\", text))\n",
    "    all_news_channels.append(channels)\n",
    "all_news_channels = [channel for channels in all_news_channels for channel in channels]\n",
    "all_news_channels = np.unique(all_news_channels, return_counts=True)\n",
    "all_news_channels = {channel:count for channel, count in zip(all_news_channels[0], all_news_channels[1])}\n",
    "all_news_channels = sorted(all_news_channels.items(), key=lambda x : x[1], reverse=True)\n",
    "\n",
    "main_news_channels = [channel[0] for channel in all_news_channels if channel[1] >= 5] \n",
    "\n",
    "# removing news channel names from news articles\n",
    "clean_texts_ = []\n",
    "for text in clean_texts:\n",
    "    for news_channel in main_news_channels:\n",
    "        text = text.replace(news_channel, \" \")\n",
    "    clean_texts_.append(text)\n",
    "clean_texts = clean_texts_\n",
    "del clean_texts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General major cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = [clean_news(text) for text in clean_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"news_cleaned\"] = clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>url</th>\n",
       "      <th>news</th>\n",
       "      <th>news_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>Today’s newsletter - Cleaning Vehicle Emissi...</td>\n",
       "      <td>https://on.wsj.com/2EuJDdb</td>\n",
       "      <td>\\n\\tNews and analysis on the world of logisti...</td>\n",
       "      <td>California is taking an aggressive step to ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>FedEx's Christmas in July; Probing Nikola Cl...</td>\n",
       "      <td>https://on.wsj.com/2Fsewzz</td>\n",
       "      <td>\\n\\tNews and analysis on the world of logisti...</td>\n",
       "      <td>FedEx is turning the surge in e-commerce deman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-14</td>\n",
       "      <td>Shipping’s E-Commerce Drive; Short-Selling N...</td>\n",
       "      <td>https://on.wsj.com/3iv2qDW</td>\n",
       "      <td>\\n\\tNews and analysis on the world of logisti...</td>\n",
       "      <td>Container shipping lines are increasingly tryi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>Brexit’s New Alarms; Railroad Offloads Bidde...</td>\n",
       "      <td>https://on.wsj.com/3inRU1m</td>\n",
       "      <td>\\n\\tNews and analysis on the world of logisti...</td>\n",
       "      <td>Fears over the fallout from a “hard” Brexit ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-23</td>\n",
       "      <td>Airlines Turn to Freight; Driving Robot Truc...</td>\n",
       "      <td>https://on.wsj.com/32TvO1a</td>\n",
       "      <td>\\n\\tNews and analysis on the world of logisti...</td>\n",
       "      <td>Just four of the world’s 30 largest passenger ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              tweet  \\\n",
       "0 2020-09-24    Today’s newsletter - Cleaning Vehicle Emissi...   \n",
       "1 2020-09-16    FedEx's Christmas in July; Probing Nikola Cl...   \n",
       "2 2020-09-14    Shipping’s E-Commerce Drive; Short-Selling N...   \n",
       "3 2020-09-10    Brexit’s New Alarms; Railroad Offloads Bidde...   \n",
       "4 2020-09-23    Airlines Turn to Freight; Driving Robot Truc...   \n",
       "\n",
       "                          url  \\\n",
       "0  https://on.wsj.com/2EuJDdb   \n",
       "1  https://on.wsj.com/2Fsewzz   \n",
       "2  https://on.wsj.com/3iv2qDW   \n",
       "3  https://on.wsj.com/3inRU1m   \n",
       "4  https://on.wsj.com/32TvO1a   \n",
       "\n",
       "                                                news  \\\n",
       "0   \\n\\tNews and analysis on the world of logisti...   \n",
       "1   \\n\\tNews and analysis on the world of logisti...   \n",
       "2   \\n\\tNews and analysis on the world of logisti...   \n",
       "3   \\n\\tNews and analysis on the world of logisti...   \n",
       "4   \\n\\tNews and analysis on the world of logisti...   \n",
       "\n",
       "                                        news_cleaned  \n",
       "0  California is taking an aggressive step to ove...  \n",
       "1  FedEx is turning the surge in e-commerce deman...  \n",
       "2  Container shipping lines are increasingly tryi...  \n",
       "3  Fears over the fallout from a “hard” Brexit ar...  \n",
       "4  Just four of the world’s 30 largest passenger ...  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"{EXCEL_PATH}\\\\wsj_logistics_news_cleaned_{from_year}-{to_year}.xlsx\"\n",
    "news_df.to_excel(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsj_tweet_scrapping",
   "language": "python",
   "name": "wsj_tweet_scrapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
