{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from os import getcwd\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import datetime as dt\n",
    "from random import randint\n",
    "from time import strptime, mktime, time\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = getcwd()[:getcwd().find(\"notebooks\")][:-1]\n",
    "path.append(f\"{PROJECT_PATH}\\\\src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chrome_driver import (\n",
    "    open_driver, \n",
    "    load_url, \n",
    "    get_driver_page_source)\n",
    "from utils import dump_pickle, load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"wsj_tweet_scrapping\"\n",
    "DATA_PATH = f\"{PROJECT_PATH}\\\\data\"\n",
    "EXCEL_PATH = f\"{DATA_PATH}\\\\excel\"\n",
    "PICKLE_PATH = f\"{DATA_PATH}\\\\pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url):\n",
    "    \"\"\"Extract text from the URL.\"\"\"\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text)\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(method=\"GET\", url=url)\n",
    "    soup = BeautifulSoup(response.data)\n",
    "    complete_texts = \"\"\n",
    "    for text in soup.find_all(name=\"p\"):\n",
    "        complete_texts += \" \" + text.text\n",
    "    return complete_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean input text.\"\"\"\n",
    "    discard_texts = [\"\\n\", \"\\t\", \"\\r\", \"\\xa0\"] \n",
    "    for word in discard_texts: \n",
    "        text = text.replace(word, \" \")\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_tweet_source(text_soup : str):\n",
    "    soup = BeautifulSoup(text_soup)\n",
    "    urls = []\n",
    "    for url in soup.find_all(name=\"a\", attrs={\"data-url\":True}):\n",
    "        url = url[\"data-url\"]\n",
    "        if \"wsj\" in url:\n",
    "            urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(start=dt.date(2009,1,1), end=dt.date(2020,9,1), freq=pd.offsets.MonthBegin(1))\n",
    "dates = dates.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter : 140, year : 2009, month : 1: 100%|████████████████████████████████████████████| 141/141 [01:57<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# processing raw tweets\n",
    "processed_tweets = []\n",
    "\n",
    "t = tqdm(range(len(dates)))\n",
    "for i in t:\n",
    "    year = dates[i].year\n",
    "    month = dates[i].month\n",
    "    t.set_description(f\"Iter : {i}, year : {year}, month : {month}\")\n",
    "    \n",
    "    # reading raw tweets\n",
    "    file_name = f\"twint_wsj_logistics_tweets_{year}_{str(month).rjust(2,'0')}\"\n",
    "    monthly_tweets = pd.read_excel(f\"{EXCEL_PATH}\\\\{file_name}.xlsx\")\n",
    "    \n",
    "    if  len(monthly_tweets) == 0:\n",
    "        continue\n",
    "        \n",
    "  \n",
    "    for i in range(len(monthly_tweets)):\n",
    "        tweet = {}\n",
    "        tweet[\"data-item-id\"] = monthly_tweets[\"data-item-id\"].iloc[i]\n",
    "        tweet[\"data-conversation-id\"] = monthly_tweets[\"data-conversation-id\"].iloc[i]\n",
    "        tweet[\"date\"] = monthly_tweets[\"date\"].iloc[i]\n",
    "        tweet[\"tweet\"] = monthly_tweets[\"tweet\"].iloc[i]\n",
    "        tweet[\"url\"] = []\n",
    "        for url in extract_urls_from_tweet_source(monthly_tweets[\"all-data\"].iloc[i]):\n",
    "            tweet[\"url\"].append(url)\n",
    "        processed_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets_df = pd.DataFrame(processed_tweets)\n",
    "cols_check = [col for col in processed_tweets_df if not isinstance(processed_tweets_df[col].iloc[0], list)]\n",
    "processed_tweets_df = processed_tweets_df.drop_duplicates(subset=cols_check).reset_index(drop = True)\n",
    "\n",
    "processed_tweets_df[\"num_urls\"] = processed_tweets_df[\"url\"].apply(lambda x: len(x))\n",
    "processed_tweets_df = processed_tweets_df[processed_tweets_df[\"num_urls\"] > 0].reset_index(drop=True)\n",
    "processed_tweets_df.drop(\"num_urls\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data-item-id</th>\n",
       "      <th>data-conversation-id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1309123013057622016</td>\n",
       "      <td>1309123013057622016</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>Today’s newsletter - Cleaning Vehicle Emissi...</td>\n",
       "      <td>[https://on.wsj.com/2EuJDdb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1306223859855753218</td>\n",
       "      <td>1306223859855753218</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>FedEx's Christmas in July; Probing Nikola Cl...</td>\n",
       "      <td>[https://on.wsj.com/2Fsewzz]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1305500577133203456</td>\n",
       "      <td>1305500577133203456</td>\n",
       "      <td>2020-09-14</td>\n",
       "      <td>Shipping’s E-Commerce Drive; Short-Selling N...</td>\n",
       "      <td>[https://on.wsj.com/3iv2qDW]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1304230721146380290</td>\n",
       "      <td>1304230721146380290</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>Brexit’s New Alarms; Railroad Offloads Bidde...</td>\n",
       "      <td>[https://on.wsj.com/3inRU1m]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1308758522033823750</td>\n",
       "      <td>1308758522033823750</td>\n",
       "      <td>2020-09-23</td>\n",
       "      <td>Airlines Turn to Freight; Driving Robot Truc...</td>\n",
       "      <td>[https://on.wsj.com/32TvO1a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>108461828237037568</td>\n",
       "      <td>108461828237037568</td>\n",
       "      <td>2011-08-30</td>\n",
       "      <td>Sina Acquires 9% Stake in Video Company Tudo...</td>\n",
       "      <td>[http://on.wsj.com/rffYru]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>108419013956222976</td>\n",
       "      <td>108419013956222976</td>\n",
       "      <td>2011-08-29</td>\n",
       "      <td>BofA Cashes Its China Chips on.wsj.com/oKfhAb\\n</td>\n",
       "      <td>[http://on.wsj.com/oKfhAb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>108232952936271872</td>\n",
       "      <td>108232952936271872</td>\n",
       "      <td>2011-08-29</td>\n",
       "      <td>S&amp;P Downgrades Sino-Forest, Withdraws Rating...</td>\n",
       "      <td>[http://on.wsj.com/pzI1nC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>108224377857912832</td>\n",
       "      <td>108224377857912832</td>\n",
       "      <td>2011-08-29</td>\n",
       "      <td>Cathay to Offer Premium Economy on.wsj.com/p...</td>\n",
       "      <td>[http://on.wsj.com/p3q984]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>108147685932015617</td>\n",
       "      <td>108147685932015617</td>\n",
       "      <td>2011-08-29</td>\n",
       "      <td>Aquino Walks a Fine Line With China on.wsj.c...</td>\n",
       "      <td>[http://on.wsj.com/mWMbup]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2478 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             data-item-id  data-conversation-id        date  \\\n",
       "0     1309123013057622016   1309123013057622016  2020-09-24   \n",
       "1     1306223859855753218   1306223859855753218  2020-09-16   \n",
       "2     1305500577133203456   1305500577133203456  2020-09-14   \n",
       "3     1304230721146380290   1304230721146380290  2020-09-10   \n",
       "4     1308758522033823750   1308758522033823750  2020-09-23   \n",
       "...                   ...                   ...         ...   \n",
       "2473   108461828237037568    108461828237037568  2011-08-30   \n",
       "2474   108419013956222976    108419013956222976  2011-08-29   \n",
       "2475   108232952936271872    108232952936271872  2011-08-29   \n",
       "2476   108224377857912832    108224377857912832  2011-08-29   \n",
       "2477   108147685932015617    108147685932015617  2011-08-29   \n",
       "\n",
       "                                                  tweet  \\\n",
       "0       Today’s newsletter - Cleaning Vehicle Emissi...   \n",
       "1       FedEx's Christmas in July; Probing Nikola Cl...   \n",
       "2       Shipping’s E-Commerce Drive; Short-Selling N...   \n",
       "3       Brexit’s New Alarms; Railroad Offloads Bidde...   \n",
       "4       Airlines Turn to Freight; Driving Robot Truc...   \n",
       "...                                                 ...   \n",
       "2473    Sina Acquires 9% Stake in Video Company Tudo...   \n",
       "2474    BofA Cashes Its China Chips on.wsj.com/oKfhAb\\n   \n",
       "2475    S&P Downgrades Sino-Forest, Withdraws Rating...   \n",
       "2476    Cathay to Offer Premium Economy on.wsj.com/p...   \n",
       "2477    Aquino Walks a Fine Line With China on.wsj.c...   \n",
       "\n",
       "                               url  \n",
       "0     [https://on.wsj.com/2EuJDdb]  \n",
       "1     [https://on.wsj.com/2Fsewzz]  \n",
       "2     [https://on.wsj.com/3iv2qDW]  \n",
       "3     [https://on.wsj.com/3inRU1m]  \n",
       "4     [https://on.wsj.com/32TvO1a]  \n",
       "...                            ...  \n",
       "2473    [http://on.wsj.com/rffYru]  \n",
       "2474    [http://on.wsj.com/oKfhAb]  \n",
       "2475    [http://on.wsj.com/pzI1nC]  \n",
       "2476    [http://on.wsj.com/p3q984]  \n",
       "2477    [http://on.wsj.com/mWMbup]  \n",
       "\n",
       "[2478 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting total number of unique urls\n",
    "total_urls = []\n",
    "for urls in processed_tweets_df[\"url\"]:\n",
    "    for url in urls:\n",
    "        total_urls.append(url)\n",
    "total_urls = np.unique(total_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2472 news\n"
     ]
    }
   ],
   "source": [
    "## incase of already extracted news\n",
    "try: \n",
    "    news_pickles = []\n",
    "    news_pickles = [pickle for pickle in glob(pathname=f\"{PICKLE_PATH}//*.pickle\") if \"wsj_logistics_news\"]\n",
    "    news = load_pickle(max(news_pickles))\n",
    "except:\n",
    "    news = {}\n",
    "print(f\"Found {len(news)} news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = open_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = {}\n",
    "url_extract_success = []\n",
    "url_extract_fail = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████████▋                                | 1420/2472 [25:42<7:37:08, 26.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed loading page, exception : Message: unknown error: net::ERR_INTERNET_DISCONNECTED\n",
      "  (Session info: MicrosoftEdge=86.0.622.51)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2472/2472 [2:04:35<00:00,  3.02s/it]\n"
     ]
    }
   ],
   "source": [
    "for url in tqdm(total_urls):\n",
    "    if url not in url_extract_success and url not in url_extract_fail:\n",
    "        try:\n",
    "            load_url(driver=driver, url=url) \n",
    "            sleep(randint(2,5))\n",
    "            soup = get_driver_page_source(driver=driver) \n",
    "            complete_texts = \"\"\n",
    "            for text in soup.find_all(name=\"p\"):\n",
    "                complete_texts += \" \" + text.text\n",
    "            news[url] = complete_texts\n",
    "            url_extract_success.append(url)\n",
    "        except Exception as e:\n",
    "            url_extract_fail.append(url)\n",
    "            print(f\"Failed extracting news, exception e : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_name = f\"wsj_logistics_news_{int(time())}\"\n",
    "dump_pickle(obj=news, filename=f\"{PICKLE_PATH}//{pickle_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Attempt to Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_extract_fail = [url for url in total_urls if len(news[url]) < 200]\n",
    "url_extract_success_attempt2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 167/167 [15:14<00:00,  5.48s/it]\n"
     ]
    }
   ],
   "source": [
    "for url in tqdm(url_extract_fail):\n",
    "    if url not in url_extract_success_attempt2:\n",
    "        load_url(driver=driver, url=url) \n",
    "        sleep(randint(2,5))\n",
    "        soup = get_driver_page_source(driver=driver) \n",
    "        complete_texts = \"\"\n",
    "        for text in soup.find_all(name=\"p\"):\n",
    "            complete_texts += \" \" + text.text\n",
    "        news[url] = complete_texts\n",
    "    else:\n",
    "        url_extract_success_attempt2.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing final news dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2478/2478 [00:01<00:00, 2227.09it/s]\n"
     ]
    }
   ],
   "source": [
    "news_df = []\n",
    "for i in tqdm(range(len(processed_tweets_df))):\n",
    "    values = processed_tweets_df.values[i]\n",
    "    row = {}\n",
    "    row[\"date\"] = values[2]\n",
    "    row[\"tweet\"] = values[3]\n",
    "    for url in values[4]:\n",
    "        row[\"url\"] = url\n",
    "        if url in news.keys():\n",
    "            row[\"news\"] = news[url]\n",
    "        else:\n",
    "            row[\"news\"] = \"\"\n",
    "        news_df.append(row)\n",
    "        \n",
    "news_df = pd.DataFrame(news_df)\n",
    "news_df[\"news\"] = news_df[\"news\"].astype(str)\n",
    "news_df[\"date\"] = pd.to_datetime(news_df[\"date\"])\n",
    "news_df = news_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_year = min(news_df[\"date\"].dt.year)\n",
    "to_year = max(news_df[\"date\"].dt.year)\n",
    "news_df.to_excel(f\"{EXCEL_PATH}\\\\wsj_logistics_news_{from_year}-{to_year}.xlsx\", \n",
    "                 index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = tqdm(range(len(dates)))\n",
    "# for i in t:\n",
    "#     year = dates[i].year\n",
    "#     month = dates[i].month\n",
    "#     t.set_description(f\"Iter : {i}, year : {year}, month : {month}\")\n",
    "    \n",
    "#     # reading raw tweets\n",
    "#     file_name = f\"twint_wsj_logistics_tweets_{year}_{str(month).rjust(2,'0')}\"\n",
    "#     monthly_tweets = pd.read_excel(f\"{EXCEL_PATH}\\\\{file_name}.xlsx\")\n",
    "    \n",
    "#     if  len(monthly_tweets) == 0:\n",
    "#         continue\n",
    "        \n",
    "#     # processing raw tweets\n",
    "#     processed_tweets = []\n",
    "#     for i in range(len(monthly_tweets)):\n",
    "#         tweet = {}\n",
    "#         tweet[\"data-item-id\"] = monthly_tweets[\"data-item-id\"].iloc[i]\n",
    "#         tweet[\"data-conversation-id\"] = monthly_tweets[\"data-conversation-id\"].iloc[i]\n",
    "#         tweet[\"date\"] = monthly_tweets[\"date\"].iloc[i]\n",
    "#         tweet[\"tweet\"] = monthly_tweets[\"tweet\"].iloc[i]\n",
    "#         for url in extract_urls_from_tweet_source(monthly_tweets[\"all-data\"].iloc[i]):\n",
    "#             tweet[\"url\"] = url\n",
    "#             processed_tweets.append(tweet)\n",
    "#     processed_tweets = pd.DataFrame(processed_tweets)\n",
    "#     processed_tweets = processed_tweets.drop_duplicates().reset_index(drop = True)\n",
    "# #     print(f\"After processing : {len(processed_tweets)} entries\")\n",
    "    \n",
    "#     if len(processed_tweets) == 0:\n",
    "#         continue\n",
    "        \n",
    "#     # fetching news articles\n",
    "#     monthly_texts = []\n",
    "#     for url in tqdm(processed_tweets[\"url\"].values):\n",
    "#         try:\n",
    "#             text = extract_text_from_url(url)\n",
    "#             text = clean_text(text)\n",
    "#             text = {\"url\": url, \"text\": text}\n",
    "#             monthly_texts.append(text) \n",
    "#             sleep(2)\n",
    "#         except Exception as e:\n",
    "#             failed_downloading_urls.append([year, month, url])\n",
    "#             print(f\"failed extracting news, exception : {e}\")\n",
    "            \n",
    "    \n",
    "#     # saving news article\n",
    "#     monthly_texts = pd.DataFrame(monthly_texts) \n",
    "#     monthly_texts = pd.merge(processed_tweets, monthly_texts, on=\"url\", how=\"left\")\n",
    "# #     print(f\"After extracting texts : {len(monthly_texts)} entries\")\n",
    "    \n",
    "#     file_name = f\"twint_wsj_logistics_news_{year}_{str(month).rjust(2,'0')}\"\n",
    "#     monthly_texts.drop_duplicates().to_excel(f\"{EXCEL_PATH}\\\\{file_name}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(failed_downloading_urls, columns=[\"year\", \"month\", \"url\"]).to_excel(f\"{EXCEL_PATH}\\\\failed_download_news_url-{int(time())}.xlsx\",\n",
    "#                                                                                  index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsj_tweet_scrapping",
   "language": "python",
   "name": "wsj_tweet_scrapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
