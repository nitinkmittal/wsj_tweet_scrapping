{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from os import getcwd\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"wsj_tweet_scrapping\"\n",
    "PROJECT_PATH = getcwd()[: getcwd().find(PROJECT_NAME) + len(PROJECT_NAME)]\n",
    "DATA_PATH = f\"{PROJECT_PATH}\\\\data\"\n",
    "EXCEL_PATH = f\"{DATA_PATH}\\\\excel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url):\n",
    "    \"\"\"Extract text from the URL.\"\"\"\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text)\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(method=\"GET\", url=url)\n",
    "    soup = BeautifulSoup(response.data)\n",
    "    complete_texts = \"\"\n",
    "    for text in soup.find_all(name=\"p\"):\n",
    "        complete_texts += \" \" + text.text\n",
    "    return complete_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean input text.\"\"\"\n",
    "    discard_texts = [\"\\n\", \"\\t\", \"\\r\", \"\\xa0\"] \n",
    "    for word in discard_texts: \n",
    "        text = text.replace(word, \" \")\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "month = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 entries\n"
     ]
    }
   ],
   "source": [
    "file_name = f\"twint_wsj_logistics_tweets_{year}_{str(month).rjust(2,'0')}\"\n",
    "monthly_tweets = pd.read_excel(f\"{EXCEL_PATH}\\\\{file_name}.xlsx\")\n",
    "print(f\"Found {len(monthly_tweets)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_tweet_source(text_soup : str):\n",
    "    soup = BeautifulSoup(text_soup)\n",
    "    urls = []\n",
    "    for url in soup.find_all(name=\"a\", attrs={\"data-url\":True}):\n",
    "        url = url[\"data-url\"]\n",
    "        if \"wsj\" in url:\n",
    "            urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 367.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processed_tweets = []\n",
    "for i in tqdm(range(len(monthly_tweets))):\n",
    "    tweet = {}\n",
    "    tweet[\"data-item-id\"] = monthly_tweets[\"data-item-id\"].iloc[i]\n",
    "    tweet[\"data-conversation-id\"] = monthly_tweets[\"data-conversation-id\"].iloc[i]\n",
    "    tweet[\"date\"] = monthly_tweets[\"date\"].iloc[i]\n",
    "    tweet[\"tweet\"] = monthly_tweets[\"tweet\"].iloc[i]\n",
    "    for url in extract_urls_from_tweet_source(monthly_tweets[\"all-data\"].iloc[i]):\n",
    "        tweet[\"url\"] = url\n",
    "        processed_tweets.append(tweet)\n",
    "        \n",
    "processed_tweets = pd.DataFrame(processed_tweets)\n",
    "processed_tweets = processed_tweets.drop_duplicates().reset_index(drop = True)\n",
    "print(f\"Found {len(processed_tweets)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data-item-id</th>\n",
       "      <th>data-conversation-id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1206966719274717184</td>\n",
       "      <td>1206966719274717184</td>\n",
       "      <td>2020-12-17</td>\n",
       "      <td>Today’s newsletter: Amazon’s Logistics Shado...</td>\n",
       "      <td>https://on.wsj.com/2M63Dn3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1211659495602044928</td>\n",
       "      <td>1211659495602044928</td>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>Today’s newsletter: The top stories shaping ...</td>\n",
       "      <td>https://on.wsj.com/2F7QzK0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1206600682691796992</td>\n",
       "      <td>1206600682691796992</td>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>Today’s Logistics Report: Competing at the C...</td>\n",
       "      <td>https://on.wsj.com/35stqxo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1205503378127765504</td>\n",
       "      <td>1205503378127765504</td>\n",
       "      <td>2020-12-13</td>\n",
       "      <td>Trading Over Tariffs; Crude’s Freight Rates;...</td>\n",
       "      <td>https://on.wsj.com/35kWwyq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1209473861655433217</td>\n",
       "      <td>1209473861655433217</td>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>Today’s Logistics Report: Delivering the Hol...</td>\n",
       "      <td>https://on.wsj.com/2ELDO7L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          data-item-id  data-conversation-id        date  \\\n",
       "0  1206966719274717184   1206966719274717184  2020-12-17   \n",
       "1  1211659495602044928   1211659495602044928  2020-12-30   \n",
       "2  1206600682691796992   1206600682691796992  2020-12-16   \n",
       "3  1205503378127765504   1205503378127765504  2020-12-13   \n",
       "4  1209473861655433217   1209473861655433217  2020-12-24   \n",
       "\n",
       "                                               tweet  \\\n",
       "0    Today’s newsletter: Amazon’s Logistics Shado...   \n",
       "1    Today’s newsletter: The top stories shaping ...   \n",
       "2    Today’s Logistics Report: Competing at the C...   \n",
       "3    Trading Over Tariffs; Crude’s Freight Rates;...   \n",
       "4    Today’s Logistics Report: Delivering the Hol...   \n",
       "\n",
       "                          url  \n",
       "0  https://on.wsj.com/2M63Dn3  \n",
       "1  https://on.wsj.com/2F7QzK0  \n",
       "2  https://on.wsj.com/35stqxo  \n",
       "3  https://on.wsj.com/35kWwyq  \n",
       "4  https://on.wsj.com/2ELDO7L  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [00:53<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "monthly_texts = []\n",
    "for url in tqdm(processed_tweets[\"url\"].values):\n",
    "    text = extract_text_from_url(url)\n",
    "    text = clean_text(text)\n",
    "    text = {\"url\": url, \"text\": text}\n",
    "    monthly_texts.append(text) \n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 entries\n"
     ]
    }
   ],
   "source": [
    "monthly_texts_df = pd.DataFrame(monthly_texts) \n",
    "monthly_texts_df = pd.merge(processed_tweets, monthly_texts_df, on=\"url\", how=\"left\")\n",
    "file_name = f\"twint_wsj_logistics_news_{year}_{str(month).rjust(2,'0')}\"\n",
    "print(f\"Found {len(monthly_texts_df)} entries\")\n",
    "monthly_texts_df.drop_duplicates().to_excel(f\"{EXCEL_PATH}\\\\{file_name}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsj_tweet_scrapping",
   "language": "python",
   "name": "wsj_tweet_scrapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
